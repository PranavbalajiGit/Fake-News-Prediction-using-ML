{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f61eb8",
   "metadata": {},
   "source": [
    "Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f824e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c554a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99d9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d2088",
   "metadata": {},
   "source": [
    "DATA COLLECTION AND PREPROCESSING \n",
    "\n",
    "About the Dataset:\n",
    "\n",
    "1. id: unique id for a news article\n",
    "2. title: the title of a news article\n",
    "3. author: author of the news article\n",
    "4. text: the text of the article; could be incomplete\n",
    "5. label: a label that marks whether the news article is real or fake:\n",
    "           1: Fake news\n",
    "           0: real News\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mention the dataset Path. Have a Own Dataset with the above columns \n",
    "news_dataset = pd.read_csv('Fake News Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66df1876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NASA discovers new exoplanet similar to Earth</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>Schools are now integrating AI models to enhan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aliens spotted in downtown New York</td>\n",
       "      <td>Sarah Davis</td>\n",
       "      <td>Eyewitnesses report bright lights and strange ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Celebrity claims to be time traveler from 3020</td>\n",
       "      <td>Olivia Walker</td>\n",
       "      <td>Scientists now admit that all space missions w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Government announces new healthcare reform</td>\n",
       "      <td>Michael Brown</td>\n",
       "      <td>NASA confirmed that the new exoplanet discover...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Local schools adopt AI-based learning programs</td>\n",
       "      <td>Sarah Davis</td>\n",
       "      <td>The discovery adds to the biodiversity of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           title         author  \\\n",
       "0   1   NASA discovers new exoplanet similar to Earth     John Smith   \n",
       "1   2             Aliens spotted in downtown New York    Sarah Davis   \n",
       "2   3  Celebrity claims to be time traveler from 3020  Olivia Walker   \n",
       "3   4      Government announces new healthcare reform  Michael Brown   \n",
       "4   5  Local schools adopt AI-based learning programs    Sarah Davis   \n",
       "\n",
       "                                                text  label  \n",
       "0  Schools are now integrating AI models to enhan...      0  \n",
       "1  Eyewitnesses report bright lights and strange ...      1  \n",
       "2  Scientists now admit that all space missions w...      1  \n",
       "3  NASA confirmed that the new exoplanet discover...      0  \n",
       "4  The discovery adds to the biodiversity of the ...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55215275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the Missing Values. \n",
    "news_dataset.isnull().sum()\n",
    "#In the Dataset i haven't given missing values because the purpose of this prediction is just to practice \n",
    "#with categorical data. So doing imputation techniques for missing values is not done here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9536435",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "751477bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       John Smith NASA discovers new exoplanet simila...\n",
      "1         Sarah Davis Aliens spotted in downtown New York\n",
      "2       Olivia Walker Celebrity claims to be time trav...\n",
      "3       Michael Brown Government announces new healthc...\n",
      "4       Sarah Davis Local schools adopt AI-based learn...\n",
      "                              ...                        \n",
      "9995    Emily Johnson Government announces new healthc...\n",
      "9996    James Lewis New species of bird discovered in ...\n",
      "9997    John Smith NASA discovers new exoplanet simila...\n",
      "9998    James Lewis Government announces new healthcar...\n",
      "9999    Michael Brown NASA discovers new exoplanet sim...\n",
      "Name: content, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Deriving a new feature that helps us in predicting the model better and constitutes to maximum variance in data. \n",
    "news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n",
    "print(news_dataset['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb23c9",
   "metadata": {},
   "source": [
    "STEMMING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fdefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Function PorterStemmer - It converts each word into its root word .\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]' , ' ' , content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stopwords.words('english')]\n",
    "    return ' '.join(stemmed_content)\n",
    "\n",
    "#Applying stemming function to each data in the content \n",
    "news_dataset['content'] = news_dataset['content'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9877eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       john smith nasa discov new exoplanet similar e...\n",
      "1                 sarah davi alien spot downtown new york\n",
      "2                  olivia walker celebr claim time travel\n",
      "3       michael brown govern announc new healthcar reform\n",
      "4       sarah davi local school adopt ai base learn pr...\n",
      "                              ...                        \n",
      "9995    emili johnson govern announc new healthcar reform\n",
      "9996    jame lewi new speci bird discov amazon rainforest\n",
      "9997    john smith nasa discov new exoplanet similar e...\n",
      "9998        jame lewi govern announc new healthcar reform\n",
      "9999    michael brown nasa discov new exoplanet simila...\n",
      "Name: content, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news_dataset['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc1707",
   "metadata": {},
   "source": [
    "SPLITTING DATA AND LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44f74eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news_dataset['content'].values\n",
    "Y = news_dataset['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cd03cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john smith nasa discov new exoplanet similar earth'\n",
      " 'sarah davi alien spot downtown new york'\n",
      " 'olivia walker celebr claim time travel' ...\n",
      " 'john smith nasa discov new exoplanet similar earth'\n",
      " 'jame lewi govern announc new healthcar reform'\n",
      " 'michael brown nasa discov new exoplanet similar earth']\n",
      "[0 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6002c6",
   "metadata": {},
   "source": [
    "VECTORIZING THE CONTENT DATA INTO NUMERICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5142f4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 55)\t0.36795235287473227\n",
      "  (0, 54)\t0.3999265761865985\n",
      "  (0, 44)\t0.22846711407657871\n",
      "  (0, 43)\t0.3999265761865985\n",
      "  (0, 35)\t0.36795235287473227\n",
      "  (0, 25)\t0.3999265761865985\n",
      "  (0, 22)\t0.31518722606174193\n",
      "  (0, 20)\t0.31282283056055166\n",
      "  (1, 66)\t0.40600973441982086\n",
      "  (1, 57)\t0.40600973441982086\n",
      "  (1, 50)\t0.377509666828921\n",
      "  (1, 44)\t0.2357907655673144\n",
      "  (1, 21)\t0.40600973441982086\n",
      "  (1, 18)\t0.377509666828921\n",
      "  (1, 2)\t0.40600973441982086\n",
      "  (2, 64)\t0.39127625873722754\n",
      "  (2, 61)\t0.4164750228696783\n",
      "  (2, 60)\t0.4164750228696783\n",
      "  (2, 45)\t0.39127625873722754\n",
      "  (2, 13)\t0.4164750228696783\n",
      "  (2, 12)\t0.4164750228696783\n",
      "  (3, 49)\t0.4176252045000466\n",
      "  (3, 44)\t0.24297114422556498\n",
      "  (3, 42)\t0.392735700472044\n",
      "  (3, 29)\t0.4176252045000466\n",
      "  :\t:\n",
      "  (9996, 8)\t0.3822073448943373\n",
      "  (9996, 3)\t0.3822073448943373\n",
      "  (9997, 55)\t0.36795235287473227\n",
      "  (9997, 54)\t0.3999265761865985\n",
      "  (9997, 44)\t0.22846711407657871\n",
      "  (9997, 43)\t0.3999265761865985\n",
      "  (9997, 35)\t0.36795235287473227\n",
      "  (9997, 25)\t0.3999265761865985\n",
      "  (9997, 22)\t0.31518722606174193\n",
      "  (9997, 20)\t0.31282283056055166\n",
      "  (9998, 49)\t0.41437900027956076\n",
      "  (9998, 44)\t0.24108252748178982\n",
      "  (9998, 39)\t0.39949419255676677\n",
      "  (9998, 34)\t0.39949419255676677\n",
      "  (9998, 29)\t0.41437900027956076\n",
      "  (9998, 28)\t0.3279605579208597\n",
      "  (9998, 6)\t0.41437900027956076\n",
      "  (9999, 54)\t0.3995323045492769\n",
      "  (9999, 44)\t0.22824187747440033\n",
      "  (9999, 43)\t0.3995323045492769\n",
      "  (9999, 42)\t0.3689274868942706\n",
      "  (9999, 25)\t0.3995323045492769\n",
      "  (9999, 22)\t0.31487649556499137\n",
      "  (9999, 20)\t0.3125144310268854\n",
      "  (9999, 10)\t0.3689274868942706\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bf244",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
